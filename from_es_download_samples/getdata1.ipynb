{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连接es，批量读取wechat数据，划分正负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    连接es\n",
    "'''\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "es = Elasticsearch('http://ip:port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    从es里读取的content是html格式，需要解析成文本\n",
    "'''\n",
    "def sentencesMaker(html):\n",
    "    sentences = []\n",
    "    if not html or not html.strip():\n",
    "        return sentences\n",
    "    try:\n",
    "        from html.parser import unescape\n",
    "        html = unescape(html)\n",
    "\n",
    "        import justext\n",
    "        paragraphs = justext.justext(html, [])\n",
    "\n",
    "        cache_sentences = ''\n",
    "\n",
    "        for p in paragraphs:\n",
    "            sent = p.text.strip().replace('\\xa0', '').replace('\\u3000', '')\n",
    "            sent = sent.encode('gb2312', 'ignore').decode('gb2312').encode('gbk', 'ignore').decode('gbk')\n",
    "            if not sent:\n",
    "                continue\n",
    "\n",
    "            # 可能是含有名字，需要进一步处理\n",
    "            if len(cache_sentences) < 5:\n",
    "                cache_sentences += ' ' + sent\n",
    "            else:\n",
    "                sentences.append(cache_sentences.strip())\n",
    "                cache_sentences = sent\n",
    "\n",
    "        if not not cache_sentences:\n",
    "            sentences.append(cache_sentences.strip())\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = [\"互访\", \"沙龙\", \"博览会\", \"展览会\", \"圆桌\", \"挂牌仪式\", \"发布会\", \"演讲\", \"组委会\", \"对接会\", \"会议\", \"理事会\", \n",
    "              \"报告\", \"分享\", \"入选\", \"培训\", \"融资\", \"大会\", \"峰会\", \"年会\", \"高峰论坛\", \"论坛\", \"研讨会\", \"比赛\", \"大赛\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(set(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of total samples from bl2: 15851\n",
      "length of total samples from ner: 15358\n",
      "length of negative samples: 7975\n",
      "length of positive samples: 7383\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    从索引wechat_bl2中获取含有主题关键字的doc的id；\n",
    "    利用获得的id去索引wechat_ner中检索该doc中的关系rel的个数是否高于自己设置的阈值，高于，正样本；低于，负样本；\n",
    "    注：从bl2中取的id，去ner中查看rel，会出现返回结果与输入id个数不一致，原因是ner在做rel操作时可能会有延迟，故按当前时刻获取的ner数据为样本；\n",
    "        bl2索引中也有rel关系，但不准，以ner索引中的为准；\n",
    "'''\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search_bl2(from_number, offsize):\n",
    "    es_search_options = set_search_optional_bl2(from_number, offsize)\n",
    "    es_result = get_search_result(es_search_options, index='wechat_bl2')\n",
    "    return es_result\n",
    "\n",
    "def search_ner(final_results):\n",
    "    es_search_options = set_search_optional_ner(final_results)\n",
    "    es_result = get_search_result(es_search_options, index='wechat_ner')\n",
    "    return es_result\n",
    "\n",
    "def get_id_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result: # [\"hits\"][\"hits\"]\n",
    "        final_result.append(item[\"_id\"])\n",
    "    return final_result\n",
    "\n",
    "def get_cleaned_content_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result:\n",
    "        final_result.append(\"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\"))\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def get_search_result(es_search_options, index, scroll='5m', doc_type='news', timeout=\"1m\"):\n",
    "    es_result = helpers.scan(\n",
    "        es,\n",
    "        query=es_search_options,\n",
    "        scroll=scroll,\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "\n",
    "def set_search_optional_bl2(from_number, offsize):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "        \"from\":from_number, \"size\":offsize,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "              \"should\": [\n",
    "                {\"match\": {\"title\": \"演讲 培训 大会\"}},\n",
    "                {\"match\": {\"content\": \"演讲 培训 大会\"}},\n",
    "              ]\n",
    "           }\n",
    "        },\n",
    "        \"_source\":{\n",
    "            \"includes\":[ \n",
    "                \"url\",\n",
    "                \"title\",\n",
    "                \"abstract\",\n",
    "                \"content\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "def set_search_optional_ner(final_results):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "def split_samples(data):\n",
    "    negative_samples = []\n",
    "    positive_samples = []\n",
    "    for i in data:\n",
    "        if not i[\"_source\"][\"rel\"] or len(i[\"_source\"][\"rel\"]) < 2:\n",
    "            negative_samples.append(i[\"_id\"])\n",
    "        else:\n",
    "            positive_samples.append(i[\"_id\"])\n",
    "    return negative_samples, positive_samples\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    all_results_bl2_1 = search_bl2(0, 5)\n",
    "    bl2_contents_results = get_cleaned_content_list(all_results_bl2_1)\n",
    "    \n",
    "    all_results_bl2_2 = search_bl2(0, 5)\n",
    "    bl2_ids_results = get_id_list(all_results_bl2_2)\n",
    "    \n",
    "    print(\"length of total samples from bl2:\", len(bl2_ids_results))\n",
    "    \n",
    "    ids_contents_dic = zip(bl2_ids_results, bl2_contents_results)\n",
    "    all_results_ner = search_ner(bl2_ids_results)\n",
    "    \n",
    "    negative_samples, positive_samples = split_samples(all_results_ner)\n",
    "    \n",
    "    print(\"length of total samples from ner:\", len(negative_samples) + len(positive_samples))\n",
    "    print(\"length of negative samples:\", len(negative_samples))\n",
    "    print(\"length of positive samples:\", len(positive_samples))\n",
    "    \n",
    "    with open(\"./test_negative_samples.txt\", \"a\", encoding=\"utf-8\") as nf, open(\"./test_positive_samples.txt\", \"a\", encoding=\"utf-8\") as pf:\n",
    "        for _id, _content  in ids_contents_dic:\n",
    "            if _id in negative_samples:\n",
    "                nf.write(\"1\" + \"\\t\" + _id + \"\\t\" +_content + \"\\n\")\n",
    "            if _id in positive_samples:\n",
    "                pf.write(\"0\" + \"\\t\" + _id + \"\\t\" + _content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_negative_samples = 1339\n",
    "# sum_positive_samples = 1275\n",
    "# total_samples = sum_negative_samples + sum_positive_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有总样本个数151485\n",
      "所有负样本个数89547，占比0.59\n",
      "所有正样本个数61938，占比0.41\n"
     ]
    }
   ],
   "source": [
    "sum_negative_samples += len(negative_samples)\n",
    "sum_positive_samples += len(positive_samples)\n",
    "total_samples = sum_negative_samples + sum_positive_samples\n",
    "print(\"所有总样本个数%d\" % total_samples)\n",
    "print(\"所有负样本个数%d，占比%.2f\" % (sum_negative_samples, (sum_negative_samples / total_samples)))\n",
    "print(\"所有正样本个数%d，占比%.2f\" % (sum_positive_samples, (sum_positive_samples / total_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\"互访 沙龙\" \n",
    "length of total samples from bl2: 844\n",
    "length of total samples from ner: 804\n",
    "length of negative samples: 397  \n",
    "length of positive samples: 407\n",
    "所有总样本个数804\n",
    "所有负样本个数397，占比0.493781\n",
    "所有正样本个数407，占比0.506219\n",
    "\n",
    "\"博览会 \"\n",
    "length of total samples from bl2: 915\n",
    "length of total samples from ner: 870\n",
    "length of negative samples: 430\n",
    "length of positive samples: 440\n",
    "所有总样本个数1674\n",
    "所有负样本个数827，占比0.494026\n",
    "所有正样本个数847，占比0.505974\n",
    "\n",
    "\"展览会\"\n",
    "length of total samples from bl2: 967\n",
    "length of total samples from ner: 940\n",
    "length of negative samples: 512\n",
    "length of positive samples: 428\n",
    "所有总样本个数2614\n",
    "所有负样本个数1339，占比0.512242\n",
    "所有正样本个数1275，占比0.487758\n",
    "\n",
    "\"圆桌\"\n",
    "length of total samples from bl2: 1012\n",
    "length of total samples from ner: 994\n",
    "length of negative samples: 314\n",
    "length of positive samples: 680\n",
    "所有总样本个数3608\n",
    "所有负样本个数1653，占比0.458149\n",
    "所有正样本个数1955，占比0.541851\n",
    "\n",
    "\"年会\"\n",
    "length of total samples from bl2: 1152\n",
    "length of total samples from ner: 1128\n",
    "length of negative samples: 466\n",
    "length of positive samples: 662\n",
    "所有总样本个数4736\n",
    "所有负样本个数2119，占比0.447424\n",
    "所有正样本个数2617，占比0.552576\n",
    "\n",
    "\"研讨会\"\n",
    "length of total samples from bl2: 1776\n",
    "length of total samples from ner: 1720\n",
    "length of negative samples: 788\n",
    "length of positive samples: 932\n",
    "所有总样本个数6456\n",
    "所有负样本个数2907，占比0.450279\n",
    "所有正样本个数3549，占比0.549721\n",
    "\n",
    "\"入选\"\n",
    "length of total samples from bl2: 1364\n",
    "length of total samples from ner: 1316\n",
    "length of negative samples: 630\n",
    "length of positive samples: 686\n",
    "所有总样本个数7772\n",
    "所有负样本个数3537，占比0.455095\n",
    "所有正样本个数4235，占比0.544905\n",
    "\n",
    "\"大赛\"\n",
    "length of total samples from bl2: 1767\n",
    "length of total samples from ner: 1689\n",
    "length of negative samples: 954\n",
    "length of positive samples: 735\n",
    "所有总样本个数9461\n",
    "所有负样本个数4491，占比0.47\n",
    "所有正样本个数4970，占比0.53\n",
    "\n",
    "\"理事会\"\n",
    "length of total samples from bl2: 1955\n",
    "length of total samples from ner: 1895\n",
    "length of negative samples: 564\n",
    "length of positive samples: 1331\n",
    "所有总样本个数11356\n",
    "所有负样本个数5055，占比0.45\n",
    "所有正样本个数6301，占比0.55\n",
    "\n",
    "\"组委会\"\n",
    "length of total samples from bl2: 2405\n",
    "length of total samples from ner: 2306\n",
    "length of negative samples: 977\n",
    "length of positive samples: 1329\n",
    "所有总样本个数13662\n",
    "所有负样本个数6032，占比0.44\n",
    "所有正样本个数7630，占比0.56\n",
    "\n",
    "\"峰会\"\n",
    "length of total samples from bl2: 3587\n",
    "length of total samples from ner: 3499\n",
    "length of negative samples: 1697\n",
    "length of positive samples: 1802\n",
    "所有总样本个数17161\n",
    "所有负样本个数7729，占比0.45\n",
    "所有正样本个数9432，占比0.55\n",
    "\n",
    "\"对接会\"\n",
    "length of total samples from bl2: 47877\n",
    "length of total samples from ner: 46478\n",
    "length of negative samples: 30533\n",
    "length of positive samples: 15945\n",
    "所有总样本个数63639\n",
    "所有负样本个数38262，占比0.60\n",
    "所有正样本个数25377，占比0.40\n",
    "\n",
    "\"发布会\"\n",
    "length of total samples from bl2: 25741\n",
    "length of total samples from ner: 24973\n",
    "length of negative samples: 15524\n",
    "length of positive samples: 9449\n",
    "所有总样本个数88612\n",
    "所有负样本个数53786，占比0.61\n",
    "所有正样本个数34826，占比0.39\n",
    "\n",
    "\"报告 分享\"\n",
    "length of total samples from bl2: 27257\n",
    "length of total samples from ner: 26413\n",
    "length of negative samples: 16715\n",
    "length of positive samples: 9698\n",
    "所有总样本个数115025\n",
    "所有负样本个数70501，占比0.61\n",
    "所有正样本个数44524，占比0.39\n",
    "\n",
    "                        \"报告\"\n",
    "                        length of total samples from bl2: 16098\n",
    "                        length of total samples from ner: 15567\n",
    "                        length of negative samples: 9711\n",
    "                        length of positive samples: 5856\n",
    "\n",
    "                        \"分享\"\n",
    "                        length of total samples from bl2: 14930\n",
    "                        length of total samples from ner: 14472\n",
    "                        length of negative samples: 8903\n",
    "                        length of positive samples: 5569\n",
    "                        \n",
    "\"挂牌仪式 会议 融资\"\n",
    "length of total samples from bl2: 21807\n",
    "length of total samples from ner: 21102\n",
    "length of negative samples: 11071\n",
    "length of positive samples: 10031\n",
    "所有总样本个数136127\n",
    "所有负样本个数81572，占比0.60\n",
    "所有正样本个数54555，占比0.40\n",
    "\n",
    "\n",
    "\"演讲 培训 大会\"\n",
    "length of total samples from bl2: 15851\n",
    "length of total samples from ner: 15358\n",
    "length of negative samples: 7975\n",
    "length of positive samples: 7383\n",
    "所有总样本个数151485\n",
    "所有负样本个数89547，占比0.59\n",
    "所有正样本个数61938，占比0.41\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples中所有文件长度： 89540\n",
      "filtered_negative_samples长度： 56839\n",
      "positive_samples中所有文件长度： 61936\n",
      "filtered_positive_samples长度： 32254\n",
      "正样本占比0.36\n",
      "负样本占比0.64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    剔除结果中重复的文章\n",
    "'''\n",
    "with open(\"./filtered_negative_samples.txt\", \"w\", encoding=\"utf-8\") as fnf, open(\"./filtered_positive_samples.txt\", \"w\", encoding=\"utf-8\") as fpf:\n",
    "    with open(\"./test_negative_samples.txt\", \"r\", encoding=\"utf-8\") as nf, open(\"./test_positive_samples.txt\", \"r\", encoding=\"utf-8\") as pf:\n",
    "\n",
    "        nf_data = nf.readlines()\n",
    "        print(\"negative_samples中所有文件长度：\", len(nf_data))\n",
    "        print(\"filtered_negative_samples长度：\", len(set(nf_data)))\n",
    "        for i in set(nf_data):\n",
    "            fnf.write(i)\n",
    "\n",
    "        pf_data = pf.readlines()\n",
    "        print(\"positive_samples中所有文件长度：\", len(pf_data))\n",
    "        print(\"filtered_positive_samples长度：\", len(set(pf_data)))\n",
    "        for j in set(pf_data):\n",
    "            fpf.write(j)\n",
    "            \n",
    "        print(\"正样本占比%.2f\" %  (len(set(pf_data))/ (len(set(pf_data)) + len(set(nf_data)))))\n",
    "        print(\"负样本占比%.2f\" %  (len(set(nf_data))/ (len(set(pf_data)) + len(set(nf_data)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩充负样本的特征，接着从toutiao里取负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results_bl2长度： 52760\n",
      "从bl2获取的id的长度： 52760\n",
      "all_results_ner长度： 52760\n",
      "length of total samples from ner: 52760\n",
      "length of negative samples: 40120\n",
      "length of positive samples: 12640\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    从头条取负样本\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search_bl2(from_number, offsize):\n",
    "    es_search_options = set_search_optional_bl2(from_number, offsize)\n",
    "    es_result = get_search_result(es_search_options, index='toutiao_bl2')\n",
    "    return es_result\n",
    "\n",
    "def search_ner(final_results):\n",
    "    es_search_options = set_search_optional_ner(final_results)\n",
    "    es_result = get_search_result(es_search_options, index='toutiao_ner')\n",
    "    return es_result\n",
    "\n",
    "def get_id_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result: # [\"hits\"][\"hits\"]\n",
    "        final_result.append(item[\"_id\"])\n",
    "    return final_result\n",
    "\n",
    "# def get_cleaned_content_list(es_result):\n",
    "#     final_result = []\n",
    "#     for item in es_result:\n",
    "#         final_result.append(\"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\"))\n",
    "#     return final_result\n",
    "\n",
    "\n",
    "def get_search_result(es_search_options, index, scroll='5m', doc_type='news', timeout=\"1m\"):\n",
    "    es_result = helpers.scan(\n",
    "        es,\n",
    "        query=es_search_options,\n",
    "        scroll=scroll,\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "\n",
    "def set_search_optional_bl2(from_number, offsize):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "      \"query\": {\n",
    "\n",
    "        \"match_all\": {}\n",
    "      },\n",
    "        \"_source\":[\"content\"]\n",
    "\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "def set_search_optional_ner(final_results):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "def split_samples(data):\n",
    "    negative_samples = []\n",
    "    positive_samples = []\n",
    "    for i in data:\n",
    "        if not i[\"_source\"][\"rel\"] or len(i[\"_source\"][\"rel\"]) < 2:\n",
    "            negative_samples.append(i[\"_id\"])\n",
    "        else:\n",
    "            positive_samples.append(i[\"_id\"])\n",
    "    return negative_samples, positive_samples\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    all_results_bl2 = search_bl2(0, 5)\n",
    "    all_results_bl2 = [item for item in all_results_bl2]  # 把从bl2获取的数据保存成list，数据里包括id,content等较少的信息\n",
    "    print(\"all_results_bl2长度：\", len(all_results_bl2))\n",
    "    \n",
    "    bl2_ids_results = get_id_list(all_results_bl2)  # 接着返回的bl2结果中，取出来ids，传给ner返回rel\n",
    "    print(\"从bl2获取的id的长度：\", len(bl2_ids_results))\n",
    "    \n",
    "    all_results_ner = search_ner(bl2_ids_results)   # ner利用bl2返回的ids的数据，取rel\n",
    "    all_results_ner = [item for item in all_results_ner]  # 把从ner获取的结果也保存成list，包括id，rel等较少的信息\n",
    "    print(\"all_results_ner长度：\", len(all_results_ner))\n",
    "    \n",
    "    \n",
    "    negative_samples, positive_samples = split_samples(all_results_ner)  # 查看ner返回的rel信息，划分正负样本的id\n",
    "    \n",
    "    print(\"length of total samples from ner:\", len(negative_samples) + len(positive_samples))\n",
    "    print(\"length of negative samples:\", len(negative_samples))\n",
    "    print(\"length of positive samples:\", len(positive_samples))\n",
    "    \n",
    "    with open(\"./toutiao_negative_samples.txt\", \"w\", encoding=\"utf-8\") as nf:  # 根据正负样本的id，去bl2返回的结果中，取回相应的content\n",
    "        for item  in all_results_bl2:\n",
    "            if item[\"_id\"] in negative_samples:\n",
    "                nf.write(\"1\" + \"\\t\" + item[\"_id\"] + \"\\t\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples中所有文件长度： 40120\n",
      "filtered_negative_samples长度： 40120\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    查看头条中的非重复的负样本数\n",
    "'''\n",
    "with open(\"./toutiao_negative_samples.txt\", \"r\", encoding=\"utf-8\") as nf:\n",
    "    nf_data = nf.readlines()\n",
    "    print(\"negative_samples中所有文件长度：\", len(nf_data))\n",
    "    print(\"filtered_negative_samples长度：\", len(set(nf_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10030\n"
     ]
    }
   ],
   "source": [
    "select_number = []\n",
    "for i in range(0, 40120, 4):\n",
    "    select_number.append(i)\n",
    "print(len(select_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    计划取10030条数据，去掉一些内容可能为空的，实际从头条取9947条数据\n",
    "'''\n",
    "with open(\"./selected_toutiao_negative_samples.txt\", \"w\", encoding=\"utf-8\") as inf:\n",
    "    with open(\"./toutiao_negative_samples.txt\", \"r\", encoding=\"utf-8\") as outf:\n",
    "        nf_data = outf.readlines()\n",
    "        for i in select_number:\n",
    "            if len(nf_data[i].strip().split('\\t')) == 3:\n",
    "                inf.write(nf_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  扩充负样本的特征，接着从baidu里取负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results_bl2长度： 45456\n",
      "从bl2获取的id的长度： 45456\n",
      "all_results_ner长度： 45391\n",
      "length of total samples from ner: 45391\n",
      "length of negative samples: 23997\n",
      "length of positive samples: 21394\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    从百度取负样本\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search_bl2(from_number, offsize):\n",
    "    es_search_options = set_search_optional_bl2(from_number, offsize)\n",
    "    es_result = get_search_result(es_search_options, index='baidu_bl2')\n",
    "    return es_result\n",
    "\n",
    "def search_ner(final_results):\n",
    "    es_search_options = set_search_optional_ner(final_results)\n",
    "    es_result = get_search_result(es_search_options, index='baidu_ner')\n",
    "    return es_result\n",
    "\n",
    "def get_id_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result: # [\"hits\"][\"hits\"]\n",
    "        final_result.append(item[\"_id\"])\n",
    "    return final_result\n",
    "\n",
    "# def get_cleaned_content_list(es_result):\n",
    "#     final_result = []\n",
    "#     for item in es_result:\n",
    "#         final_result.append(\"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\"))\n",
    "#     return final_result\n",
    "\n",
    "\n",
    "def get_search_result(es_search_options, index, scroll='5m', doc_type='news', timeout=\"1m\"):\n",
    "    es_result = helpers.scan(\n",
    "        es,\n",
    "        query=es_search_options,\n",
    "        scroll=scroll,\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "\n",
    "def set_search_optional_bl2(from_number, offsize):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "      \"query\": {\n",
    "\n",
    "        \"match_all\": {}\n",
    "      },\n",
    "        \"_source\":[\"content\"]\n",
    "\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "def set_search_optional_ner(final_results):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "def split_samples(data):\n",
    "    negative_samples = []\n",
    "    positive_samples = []\n",
    "    for i in data:\n",
    "        if not i[\"_source\"][\"rel\"] or len(i[\"_source\"][\"rel\"]) < 2:\n",
    "            negative_samples.append(i[\"_id\"])\n",
    "        else:\n",
    "            positive_samples.append(i[\"_id\"])\n",
    "    return negative_samples, positive_samples\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    all_results_bl2 = search_bl2(0, 5)\n",
    "    all_results_bl2 = [item for item in all_results_bl2]  # 把从bl2获取的数据保存成list，数据里包括id,content等较少的信息\n",
    "    print(\"all_results_bl2长度：\", len(all_results_bl2))\n",
    "    \n",
    "    bl2_ids_results = get_id_list(all_results_bl2)  # 接着返回的bl2结果中，取出来ids，传给ner返回rel\n",
    "    print(\"从bl2获取的id的长度：\", len(bl2_ids_results))\n",
    "    \n",
    "    all_results_ner = search_ner(bl2_ids_results)   # ner利用bl2返回的ids的数据，取rel\n",
    "    all_results_ner = [item for item in all_results_ner]  # 把从ner获取的结果也保存成list，包括id，rel等较少的信息\n",
    "    print(\"all_results_ner长度：\", len(all_results_ner))\n",
    "    \n",
    "    \n",
    "    negative_samples, positive_samples = split_samples(all_results_ner)  # 查看ner返回的rel信息，划分正负样本的id\n",
    "    \n",
    "    print(\"length of total samples from ner:\", len(negative_samples) + len(positive_samples))\n",
    "    print(\"length of negative samples:\", len(negative_samples))\n",
    "    print(\"length of positive samples:\", len(positive_samples))\n",
    "    \n",
    "    with open(\"./baidu_negative_samples.txt\", \"w\", encoding=\"utf-8\") as nf:  # 根据正负样本的id，去bl2返回的结果中，取回相应的content\n",
    "        for item  in all_results_bl2:\n",
    "            if item[\"_id\"] in negative_samples:\n",
    "                nf.write(\"1\" + \"\\t\" + item[\"_id\"] + \"\\t\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples中所有文件长度： 23997\n",
      "filtered_negative_samples长度： 23997\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    查看百度中的非重复的负样本数\n",
    "'''\n",
    "with open(\"./baidu_negative_samples.txt\", \"r\", encoding=\"utf-8\") as nf:\n",
    "    nf_data = nf.readlines()\n",
    "    print(\"negative_samples中所有文件长度：\", len(nf_data))\n",
    "    print(\"filtered_negative_samples长度：\", len(set(nf_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n"
     ]
    }
   ],
   "source": [
    "select_number = []\n",
    "for i in range(0, 23997, 2):\n",
    "    select_number.append(i)\n",
    "print(len(select_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    计划取11999条数据，去掉一些内容可能为空的，实际从头条取9947条数据\n",
    "'''\n",
    "with open(\"./selected_baidu_negative_samples.txt\", \"w\", encoding=\"utf-8\") as inf:\n",
    "    with open(\"./baidu_negative_samples.txt\", \"r\", encoding=\"utf-8\") as outf:\n",
    "        nf_data = outf.readlines()\n",
    "        count = 0\n",
    "        for i in select_number:\n",
    "            if len(nf_data[i].strip().split('\\t')) == 3:\n",
    "                inf.write(nf_data[i])\n",
    "                count += 1\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33792"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wechat + baidu + toutiao 获取的负样本数\n",
    "11999 + 9947 + 11846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36524"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从36kr获取2732条负样本\n",
    "33792+2732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12738"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  扩充负样本的特征，接着从36kr里取负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results_bl2长度： 4970\n",
      "从bl2获取的id的长度： 4970\n",
      "all_results_ner长度： 4970\n",
      "length of total samples from ner: 4970\n",
      "length of negative samples: 2732\n",
      "length of positive samples: 2238\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    从百度取负样本\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search_bl2(from_number, offsize):\n",
    "    es_search_options = set_search_optional_bl2(from_number, offsize)\n",
    "    es_result = get_search_result(es_search_options, index='36kr_bl2')\n",
    "    return es_result\n",
    "\n",
    "def search_ner(final_results):\n",
    "    es_search_options = set_search_optional_ner(final_results)\n",
    "    es_result = get_search_result(es_search_options, index='36kr_ner')\n",
    "    return es_result\n",
    "\n",
    "def get_id_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result: # [\"hits\"][\"hits\"]\n",
    "        final_result.append(item[\"_id\"])\n",
    "    return final_result\n",
    "\n",
    "# def get_cleaned_content_list(es_result):\n",
    "#     final_result = []\n",
    "#     for item in es_result:\n",
    "#         final_result.append(\"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\"))\n",
    "#     return final_result\n",
    "\n",
    "\n",
    "def get_search_result(es_search_options, index, scroll='5m', doc_type='news', timeout=\"1m\"):\n",
    "    es_result = helpers.scan(\n",
    "        es,\n",
    "        query=es_search_options,\n",
    "        scroll=scroll,\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "\n",
    "def set_search_optional_bl2(from_number, offsize):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "      \"query\": {\n",
    "\n",
    "        \"match_all\": {}\n",
    "      },\n",
    "        \"_source\":[\"content\"]\n",
    "\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "def set_search_optional_ner(final_results):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "def split_samples(data):\n",
    "    negative_samples = []\n",
    "    positive_samples = []\n",
    "    for i in data:\n",
    "        if not i[\"_source\"][\"rel\"] or len(i[\"_source\"][\"rel\"]) < 2:\n",
    "            negative_samples.append(i[\"_id\"])\n",
    "        else:\n",
    "            positive_samples.append(i[\"_id\"])\n",
    "    return negative_samples, positive_samples\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    all_results_bl2 = search_bl2(0, 5)\n",
    "    all_results_bl2 = [item for item in all_results_bl2]  # 把从bl2获取的数据保存成list，数据里包括id,content等较少的信息\n",
    "    print(\"all_results_bl2长度：\", len(all_results_bl2))\n",
    "    \n",
    "    bl2_ids_results = get_id_list(all_results_bl2)  # 接着返回的bl2结果中，取出来ids，传给ner返回rel\n",
    "    print(\"从bl2获取的id的长度：\", len(bl2_ids_results))\n",
    "    \n",
    "    all_results_ner = search_ner(bl2_ids_results)   # ner利用bl2返回的ids的数据，取rel\n",
    "    all_results_ner = [item for item in all_results_ner]  # 把从ner获取的结果也保存成list，包括id，rel等较少的信息\n",
    "    print(\"all_results_ner长度：\", len(all_results_ner))\n",
    "    \n",
    "    \n",
    "    negative_samples, positive_samples = split_samples(all_results_ner)  # 查看ner返回的rel信息，划分正负样本的id\n",
    "    \n",
    "    print(\"length of total samples from ner:\", len(negative_samples) + len(positive_samples))\n",
    "    print(\"length of negative samples:\", len(negative_samples))\n",
    "    print(\"length of positive samples:\", len(positive_samples))\n",
    "    \n",
    "    with open(\"./36kr_negative_samples.txt\", \"w\", encoding=\"utf-8\") as nf:  # 根据正负样本的id，去bl2返回的结果中，取回相应的content\n",
    "        for item  in all_results_bl2:\n",
    "            if item[\"_id\"] in negative_samples:\n",
    "                nf.write(\"1\" + \"\\t\" + item[\"_id\"] + \"\\t\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples中所有文件长度： 2732\n",
      "filtered_negative_samples长度： 2732\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    查看百度中的非重复的负样本数\n",
    "'''\n",
    "with open(\"./36kr_negative_samples.txt\", \"r\", encoding=\"utf-8\") as nf:\n",
    "    nf_data = nf.readlines()\n",
    "    print(\"negative_samples中所有文件长度：\", len(nf_data))\n",
    "    print(\"filtered_negative_samples长度：\", len(set(nf_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2732\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    计划取全部的36kr负样本，实际从头条取9947条数据\n",
    "'''\n",
    "with open(\"./selected_36kr_negative_samples.txt\", \"w\", encoding=\"utf-8\") as inf:\n",
    "    with open(\"./36kr_negative_samples.txt\", \"r\", encoding=\"utf-8\") as outf:\n",
    "        nf_data = outf.readlines()\n",
    "        count = 0\n",
    "        for i in nf_data:\n",
    "            if len(i.strip().split('\\t')) == 3:\n",
    "                inf.write(i)\n",
    "                count += 1\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将从toutiao，baidu，36kr，wechat取得的非空非重复的负样本合并，并出除id列\n",
    "### 将从wechat取得到正样本换成统一名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./selected_36kr_negative_samples.txt\", \"r\", encoding=\"utf-8\") as kr:\n",
    "    with open(\"./selected_baidu_negative_samples.txt\", \"r\", encoding=\"utf-8\") as baidu:\n",
    "        with open(\"./selected_toutiao_negative_samples.txt\", \"r\", encoding=\"utf-8\") as toutiao:\n",
    "            with open(\"./selected_wechat_negative_samples.txt\", \"r\", encoding=\"utf-8\") as wechat:\n",
    "                with open(\"./total_negative_samples.txt\", \"w\", encoding=\"utf-8\") as negative:\n",
    "                    data_kr = kr.readlines()\n",
    "                    data_baidu = baidu.readlines()\n",
    "                    data_toutiao = toutiao.readlines()\n",
    "                    data_wechat = wechat.readlines()\n",
    "                    for line in data_kr:\n",
    "                        negative.write(line.split(\"\\t\")[0] + \"\\t\" + line.split(\"\\t\")[2])\n",
    "                    for line in data_baidu:\n",
    "                        negative.write(line.split(\"\\t\")[0] + \"\\t\" + line.split(\"\\t\")[2])\n",
    "                    for line in data_toutiao:\n",
    "                        negative.write(line.split(\"\\t\")[0] + \"\\t\" + line.split(\"\\t\")[2])\n",
    "                    for line in data_wechat:\n",
    "                        negative.write(line.split(\"\\t\")[0] + \"\\t\" + line.split(\"\\t\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36524\n"
     ]
    }
   ],
   "source": [
    " with open(\"./total_negative_samples.txt\", \"r\", encoding=\"utf-8\") as negative:\n",
    "        data = negative.readlines()\n",
    "        print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./filtered_wechat_positive_samples.txt\", \"r\", encoding=\"utf-8\") as wechat:\n",
    "    with open(\"./total_positive_samples.txt\", \"w\", encoding=\"utf-8\") as positive:\n",
    "        data_wechat = wechat.readlines()\n",
    "        for line in data_wechat:\n",
    "            positive.write(line.split(\"\\t\")[0] + \"\\t\" + line.split(\"\\t\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12738\n"
     ]
    }
   ],
   "source": [
    " with open(\"./total_positive_samples.txt\", \"r\", encoding=\"utf-8\") as positive:\n",
    "        data = positive.readlines()\n",
    "        print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取的ids和contents数量不一致问题\n",
    "#### 因为从es获得的数据是generator形式，为了获取id和content就先后两次连接es，造成返回的结果数量不一致，接着后面再用zip将\n",
    "#### 数量不一致的id和content拼接，可能更错。\n",
    "#### 改正： 一次获取的es数据，存成list格式；避免使用zip，直接根据id返回对应的content\n",
    "#### 但是根据bl2的id去ner获取数据时，可能出现数据不一致，因为ner在解析时可能有延迟，但是只要保证id和content对应起来就可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52701\n",
      "52708\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    从36kr获取数据实验，36kr数据量较少，4933条左右\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search_bl2(from_number, offsize):\n",
    "    es_search_options = set_search_optional_bl2(from_number, offsize)\n",
    "    es_result = get_search_result(es_search_options, index='toutiao_bl2')\n",
    "    return es_result\n",
    "\n",
    "def search_ner(final_results):\n",
    "    es_search_options = set_search_optional_ner(final_results)\n",
    "    es_result = get_search_result(es_search_options, index='toutiao_ner')\n",
    "    return es_result\n",
    "\n",
    "def get_id_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result: # [\"hits\"][\"hits\"]\n",
    "        final_result.append(item[\"_id\"])\n",
    "    return final_result\n",
    "\n",
    "def get_cleaned_content_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result:\n",
    "        final_result.append(\"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\"))\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def get_search_result(es_search_options, index, scroll='5m', doc_type='news', timeout=\"1m\"):\n",
    "    es_result = helpers.scan(\n",
    "        es,\n",
    "        query=es_search_options,\n",
    "        scroll=scroll,\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "\n",
    "def set_search_optional_bl2(from_number, offsize):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "      \"query\": {\n",
    "\n",
    "        \"match_all\": {}\n",
    "      }\n",
    "\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "def set_search_optional_ner(final_results):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "def split_samples(data):\n",
    "    negative_samples = []\n",
    "    positive_samples = []\n",
    "    for i in data:\n",
    "        if not i[\"_source\"][\"rel\"] or len(i[\"_source\"][\"rel\"]) < 2:\n",
    "            negative_samples.append(i[\"_id\"])\n",
    "        else:\n",
    "            positive_samples.append(i[\"_id\"])\n",
    "    return negative_samples, positive_samples\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    all_results_bl2_1 = search_bl2(0, 5)\n",
    "    \n",
    "    j  = 0\n",
    "    for i in all_results_bl2_1:\n",
    "        j  += 1\n",
    "    print(j)\n",
    "        \n",
    "#     bl2_contents_results = get_cleaned_content_list(all_results_bl2_1)\n",
    "#     print(len(bl2_contents_results))\n",
    "    \n",
    "    all_results_bl2_2 = search_bl2(0, 5)\n",
    "    \n",
    "    k = 0\n",
    "    for i in all_results_bl2_2:\n",
    "        k += 1\n",
    "    print(k)\n",
    "        \n",
    "#     bl2_ids_results = get_id_list(all_results_bl2_2)\n",
    "#     print(len(bl2_ids_results))\n",
    "    \n",
    "#     print(\"length of total samples from bl2:\", len(bl2_ids_results))\n",
    "    \n",
    "#     ids_contents_dic = zip(bl2_ids_results, bl2_contents_results)\n",
    "    \n",
    "#     all_results_ner = search_ner(bl2_ids_results)\n",
    "    \n",
    "#     negative_samples, positive_samples = split_samples(all_results_ner)\n",
    "    \n",
    "#     print(\"length of total samples from ner:\", len(negative_samples) + len(positive_samples))\n",
    "#     print(\"length of negative samples:\", len(negative_samples))\n",
    "#     print(\"length of positive samples:\", len(positive_samples))\n",
    "    \n",
    "#     with open(\"./toutiao_negative_samples.txt\", \"w\", encoding=\"utf-8\") as nf:\n",
    "#         for _id, _content  in ids_contents_dic:\n",
    "#             if _id in negative_samples:\n",
    "#                 nf.write(\"1\" + \"\\t\" + _id + \"\\t\" +_content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# import es_client\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search(final_results):\n",
    "    es_search_options = set_search_optional(final_results)\n",
    "    es_result = get_search_result(es_search_options)\n",
    "    final_result = get_result_list(es_result)\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def get_result_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result: # [\"hits\"][\"hits\"]\n",
    "        final_result.append(item[\"_source\"][\"rel\"])\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def get_search_result(es_search_options, scroll='5m', index='wechat_ner', doc_type='news', timeout=\"1m\"):\n",
    "    es_result = helpers.scan(\n",
    "        es,\n",
    "        query=es_search_options,\n",
    "        scroll=scroll,\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "\n",
    "def set_search_optional(final_results):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    final_results = search(final_results)\n",
    "    print(len(final_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
