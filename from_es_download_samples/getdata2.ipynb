{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试 es.search和helpers.scan能否批量输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    连接es\n",
    "'''\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "es = Elasticsearch('http://ip:port')\n",
    "\n",
    "doc_index = 'wechat_bl2'\n",
    "doc_type = 'news'\n",
    "\n",
    "query = {\n",
    "  \"from\":0, \"size\":2,\n",
    "  \"query\": {\n",
    "    \"multi_match\": {\n",
    "      \"query\": \"互访\",\n",
    "      \"fields\": [\"title^4\", \"content\"],\n",
    "      \"analyzer\": \"ik_max_word\"\n",
    "    }\n",
    "  },\n",
    "  \"_source\": {\n",
    "    \"includes\": \"url\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# es.search可以批量输出\n",
    "# helpers.scan尽管设置了size，仍是全部输出\n",
    "\n",
    "full_data2 = es.search(index=doc_index, doc_type=doc_type, body=query) \n",
    "# data = helpers.scan(es, query=query, index=doc_index, doc_type=doc_type, scroll=\"1m\",timeout='1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_index': 'wechat_bl2', '_type': 'news', '_id': 'MzA4NDA4OTgwMQ==_2650766157_1', '_score': 14.735005, '_source': {'url': 'http://mp.weixin.qq.com/s?__biz=MzA4NDA4OTgwMQ==&mid=2650766157&idx=1&sn=ff4238d496937defeda1aaafe25f3083&chksm=87e755ebb090dcfdf6bcef7254331149b9f7afd45d86d5bc0d86eb19843ffc8f4793b6db701f&scene=27'}}\n"
     ]
    }
   ],
   "source": [
    "print(full_data2[\"hits\"][\"hits\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试将连接es返回的generator型结果，转成list，长久保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    从es里读取的content是html格式，需要解析成文本\n",
    "'''\n",
    "def sentencesMaker(html):\n",
    "    sentences = []\n",
    "    if not html or not html.strip():\n",
    "        return sentences\n",
    "    try:\n",
    "        from html.parser import unescape\n",
    "        html = unescape(html)\n",
    "\n",
    "        import justext\n",
    "        paragraphs = justext.justext(html, [])\n",
    "\n",
    "        cache_sentences = ''\n",
    "\n",
    "        for p in paragraphs:\n",
    "            sent = p.text.strip().replace('\\xa0', '').replace('\\u3000', '')\n",
    "            sent = sent.encode('gb2312', 'ignore').decode('gb2312').encode('gbk', 'ignore').decode('gbk')\n",
    "            if not sent:\n",
    "                continue\n",
    "\n",
    "            # 可能是含有名字，需要进一步处理\n",
    "            if len(cache_sentences) < 5:\n",
    "                cache_sentences += ' ' + sent\n",
    "            else:\n",
    "                sentences.append(cache_sentences.strip())\n",
    "                cache_sentences = sent\n",
    "\n",
    "        if not not cache_sentences:\n",
    "            sentences.append(cache_sentences.strip())\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    从36kr获取数据实验，36kr数据量较少，4933条左右\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search_bl2(from_number, offsize):\n",
    "    es_search_options = set_search_optional_bl2(from_number, offsize)\n",
    "    es_result = get_search_result(es_search_options, index='36kr_bl2')\n",
    "    return es_result\n",
    "\n",
    "def search_ner(final_results):\n",
    "    es_search_options = set_search_optional_ner(final_results)\n",
    "    es_result = get_search_result(es_search_options, index='36kr_ner')\n",
    "    return es_result\n",
    "\n",
    "def get_id_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result: # [\"hits\"][\"hits\"]\n",
    "        final_result.append(item[\"_id\"])\n",
    "    return final_result\n",
    "\n",
    "def get_cleaned_content_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result:\n",
    "        final_result.append(\"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\"))\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def get_search_result(es_search_options, index, scroll='5m', doc_type='news', timeout=\"1m\"):\n",
    "    es_result = helpers.scan(\n",
    "        es,\n",
    "        query=es_search_options,\n",
    "        scroll=scroll,\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "\n",
    "def set_search_optional_bl2(from_number, offsize):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "      \"query\": {\n",
    "\n",
    "        \"match_all\": {}\n",
    "      },\n",
    "        \"_source\":[\"content\"]\n",
    "\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "def set_search_optional_ner(final_results):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "def split_samples(data):\n",
    "    negative_samples = []\n",
    "    positive_samples = []\n",
    "    for i in data:\n",
    "        if not i[\"_source\"][\"rel\"] or len(i[\"_source\"][\"rel\"]) < 2:\n",
    "            negative_samples.append(i[\"_id\"])\n",
    "        else:\n",
    "            positive_samples.append(i[\"_id\"])\n",
    "    return negative_samples, positive_samples\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    all_results_bl2_1 = search_bl2(0, 5)\n",
    "    all_results_bl2_1 = [item for item in all_results_bl2_1]\n",
    "    \n",
    "#     j  = 0\n",
    "#     for i in all_results_bl2_1:\n",
    "#         j  += 1\n",
    "#     print(j)\n",
    "        \n",
    "#     bl2_contents_results = get_cleaned_content_list(all_results_bl2_1)\n",
    "#     all_results_bl2_1 = [item for item in all_results_bl2_1]\n",
    "#     print(len(bl2_contents_results))\n",
    "    \n",
    "#     all_results_bl2_2 = search_bl2(0, 5)\n",
    "    \n",
    "#     k = 0\n",
    "#     for i in all_results_bl2_2:\n",
    "#         k += 1\n",
    "#     print(k)\n",
    "        \n",
    "    bl2_ids_results = get_id_list(all_results_bl2_2)\n",
    "    print(len(bl2_ids_results))\n",
    "    \n",
    "#     print(\"length of total samples from bl2:\", len(bl2_ids_results))\n",
    "    \n",
    "#     ids_contents_dic = zip(bl2_ids_results, bl2_contents_results)\n",
    "    \n",
    "#     all_results_ner = search_ner(bl2_ids_results)\n",
    "    \n",
    "#     negative_samples, positive_samples = split_samples(all_results_ner)\n",
    "    \n",
    "#     print(\"length of total samples from ner:\", len(negative_samples) + len(positive_samples))\n",
    "#     print(\"length of negative samples:\", len(negative_samples))\n",
    "#     print(\"length of positive samples:\", len(positive_samples))\n",
    "    \n",
    "#     with open(\"./toutiao_negative_samples.txt\", \"w\", encoding=\"utf-8\") as nf:\n",
    "#         for _id, _content  in ids_contents_dic:\n",
    "#             if _id in negative_samples:\n",
    "#                 nf.write(\"1\" + \"\\t\" + _id + \"\\t\" +_content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4933\n"
     ]
    }
   ],
   "source": [
    "bl2_ids_results = get_id_list(all_results_bl2_1)\n",
    "print(len(bl2_ids_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5116737\n"
     ]
    }
   ],
   "source": [
    "print(bl2_ids_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_results_bl2_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4933\n"
     ]
    }
   ],
   "source": [
    "print(len(all_results_bl2_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_index': '36kr_bl2', '_type': 'news', '_id': '5116737', '_score': None, '_source': {'content': '<figure><img src=\"https://pic.36krcnd.com/201801/30011449/rrt6lha92dha3p38!heading\"></figure>\\n<p>以比特币为代表的区块链1.0时代，解决了点对点价值传输中的信任问题，但比特币逐渐成为了“投资品”，其流通和支付的属性越来越弱；到了区块链2.0时代，以太坊通过智能合约实现了不同场景的应用，但共用一条主链容易造成网络拥堵，甚至瘫痪。<br/></p><p>在底层设计上，比特币和以太坊分别对应确定有限和确定增量的区块，两者的“过度共识”无法承载社会规模的生产。</p><p>如果把比特币比作“一个点”、以太坊比作“一条线”，那么黑派科技想站在构建完整生态的角度设计“一个面”。黑派科技于去年11月成为新加坡ValueCyber中国区技术合作伙伴，目前正在开发的区块链3.0项目ValueCyber，<strong>为“去中心化”的应用和应用间的协作提供工具和平台。</strong></p><p><strong>ValueCyber通过“价值-债务网络”的集体共识机制，实现多链、多系统、多场景的互联；同时根据接入的生产网络的流动性需求，自动调整token总量，保持货币的“弹性”与价值的相对恒定。</strong></p><p>在技术方面，ValueCyber采用分层结构解决“过度共识”的问题，即允许每个应用根据自身需求，形成独立的链，并连接到ValueCyber主链。当单一应用需要“强共识”时，可与外界跨链协作，在大部分不需要跨链的时间，只需链内解决问题。</p><p>ValueCyber结合了以太坊中智能合约的思想，以及比特币区块链的安全性（防止算力攻击），同时向上开放满足通用性，向下兼容主流公有链。开发者可直接接入ValueCyber的跨链协议，降低迁移成本。</p><p><span><img alt=\"区块链应用 | \\u200b将区块链技术映射到实体经济？「ValueCyber」想成为下一代区块链底层\" data-img-size-val=\"916,449\" data-src=\"https://pic.36krcnd.com/avatar/201801/30012559/zlofv5nx8rthj5sg.png!1200\" onload=\"loadHtmlImg(this)\" style=\"width: 720px; height: 352.92576419213975px\"/></span></p><p><span>技术架构图：左侧是与物理世界相连的准入系统，中间是ValueCyber提供的各条链和跨链协议，右侧是目前市场上常见的主链</span></p><p>此外，<strong>ValueCyber的创新之处是用“债务网络”与实体经济产生关联。“债务网络”模拟了传统经济学模型中的供需关系。通过引入债务，加强企业资金的短期流动性，并解决生产力增减导致的通缩通胀。</strong></p><p>举个例子，需要资金的公司可以在ValueCyber上抵押其token，换取ValueCyber官方token（VCT），来增强资金的流动性，这类似商业承兑汇票的场外交易。</p><p>VCT的总量根据系统内总经济价值的增减动态调整。因此，在ValueCyber的生态中，博弈机制成为了“中心”，token的价值相对恒定（像USDT一样）。这种“弹性”保证投机者无法炒币牟利，也保护了债务双方利益。</p><p>市面上希望成为下一代区块链底层的，既有EOS这种扩展智能合约的，也有靠分叉提升区块链容量的。众多的公有链都希望满足承载大量用户、降低使用门槛、适配多种场景几个特点。黑派科技认为ValueCyber不是单就某一点升级，而是重塑底层。</p><p>据介绍，ValueCyber主链的开发周期在6-10个月间，之后将完成从工具到应用、从社群到生态的演变。</p><p>团队现有20余人，有丰富的P2P网络研发经验，8年前起接触区块链技术，见证并参与“币圈”、“链圈”在中国的发展过程，以及区块链技术在各行业应用的试水和落地。创始人李茗，乐视集团早期核心团队成员，曾任乐视云高级总监、太一云VP，国内首批数字货币开发者，拥有27项已授权技术发明专利。CEO郭鹏举，原太一云VP、元宝网产品负责人。</p><p><em>我是郝方舟，关注区块链相关优质项目，加微信nooxika请备注公司+姓名+事由。</em></p>'}, 'sort': [18]}\n"
     ]
    }
   ],
   "source": [
    "print(all_results_bl2_1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按主题词从es里重新获取wechat数据，划分正负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = [\"互访\", \"沙龙\", \"博览会\", \"展览会\", \"圆桌\", \"挂牌仪式\", \"发布会\", \"演讲\", \"组委会\", \"对接会\", \"会议\", \"理事会\", \n",
    "              \"报告\", \"分享\", \"入选\", \"培训\", \"融资\", \"大会\", \"峰会\", \"年会\", \"高峰论坛\", \"论坛\", \"研讨会\", \"比赛\", \"大赛\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results_bl2长度： 3184\n",
      "从bl2获取的id的长度： 3184\n",
      "all_results_ner长度： 3183\n",
      "length of total samples from ner: 3183\n",
      "length of negative samples: 2357\n",
      "length of positive samples: 826\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    从wechat取正、负样本\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search_bl2(from_number, off_size):\n",
    "    es_search_options = set_search_optional_bl2(from_number, off_size)\n",
    "    es_result = get_search_result(es_search_options, index='wechat_bl2')\n",
    "    return es_result\n",
    "\n",
    "def search_ner(final_results, from_number, off_size):\n",
    "    es_search_options = set_search_optional_ner(final_results, from_number, off_size)\n",
    "    es_result = get_search_result(es_search_options, index='wechat_ner')\n",
    "    return es_result\n",
    "\n",
    "def get_id_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result: # [\"hits\"][\"hits\"]\n",
    "        final_result.append(item[\"_id\"])\n",
    "    return final_result\n",
    "\n",
    "# def get_cleaned_content_list(es_result):\n",
    "#     final_result = []\n",
    "#     for item in es_result:\n",
    "#         final_result.append(\"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\"))\n",
    "#     return final_result\n",
    "\n",
    "\n",
    "def get_search_result(es_search_options, index, scroll='5m', doc_type='news', timeout=\"1m\"):\n",
    "    es_result = helpers.scan(\n",
    "        es,\n",
    "        query=es_search_options,\n",
    "        scroll=scroll,\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "\n",
    "def set_search_optional_bl2(from_number, off_size):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "        \"from\":from_number, \"size\":off_size,\n",
    "        \"query\": {\n",
    "            \"analyzer\": \"ik_max_word\"\n",
    "            \"bool\": {\n",
    "              \"should\": [\n",
    "                {\"terms\": topic_words},\n",
    "                {\"terms\": topic_words},\n",
    "              ]\n",
    "           }\n",
    "        },\n",
    "        \"_source\":[\"content\"]\n",
    "    \n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "def set_search_optional_ner(final_results, from_number, off_size):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "        \"from\":from_number, \"size\":off_size,\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }\n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "def split_samples(data):\n",
    "    negative_samples = []\n",
    "    positive_samples = []\n",
    "    for i in data:\n",
    "        if not i[\"_source\"][\"rel\"] or len(i[\"_source\"][\"rel\"]) < 3:\n",
    "            negative_samples.append(i[\"_id\"])\n",
    "        else:\n",
    "            positive_samples.append(i[\"_id\"])\n",
    "    return negative_samples, positive_samples\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    all_results_bl2 = search_bl2(0,5)\n",
    "    all_results_bl2 = [item for item in all_results_bl2]  # 把从bl2获取的数据保存成list，数据里包括id,content等较少的信息\n",
    "    print(\"all_results_bl2长度：\", len(all_results_bl2))\n",
    "    \n",
    "    bl2_ids_results = get_id_list(all_results_bl2)  # 接着返回的bl2结果中，取出来ids，传给ner返回rel\n",
    "    print(\"从bl2获取的id的长度：\", len(bl2_ids_results))\n",
    "    \n",
    "    all_results_ner = search_ner(bl2_ids_results, 0, 5)   # ner利用bl2返回的ids的数据，取rel\n",
    "    all_results_ner = [item for item in all_results_ner]  # 把从ner获取的结果也保存成list，包括id，rel等较少的信息\n",
    "    print(\"all_results_ner长度：\", len(all_results_ner))\n",
    "    \n",
    "    \n",
    "    negative_samples, positive_samples = split_samples(all_results_ner)  # 查看ner返回的rel信息，划分正负样本的id\n",
    "    \n",
    "    print(\"length of total samples from ner:\", len(negative_samples) + len(positive_samples))\n",
    "    print(\"length of negative samples:\", len(negative_samples))\n",
    "    print(\"length of positive samples:\", len(positive_samples))\n",
    "    \n",
    "    # 根据正负样本的id，去bl2返回的结果中，取回相应的content\n",
    "    with open(\"./wechat_negative_samples.txt\", \"a\", encoding=\"utf-8\") as nf, open(\"./wechat_positive_samples.txt\", \"a\", encoding=\"utf-8\") as pf:  \n",
    "        for item in all_results_bl2:\n",
    "            if item[\"_id\"] in negative_samples:\n",
    "                nf.write(\"1\" + \"\\t\" + item[\"_id\"] + \"\\t\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")\n",
    "            if item[\"_id\"] in positive_samples:\n",
    "                pf.write(\"0\" + \"\\t\" + item[\"_id\"] + \"\\t\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有总样本个数184438\n",
      "所有负样本个数128185，占比0.70\n",
      "所有正样本个数56253，占比0.30\n"
     ]
    }
   ],
   "source": [
    "# sum_negative_samples, sum_positive_samples, total_samples = 0, 0, 0\n",
    "sum_negative_samples += len(negative_samples)\n",
    "sum_positive_samples += len(positive_samples)\n",
    "total_samples = sum_negative_samples + sum_positive_samples\n",
    "print(\"所有总样本个数%d\" % total_samples)\n",
    "print(\"所有负样本个数%d，占比%.2f\" % (sum_negative_samples, (sum_negative_samples / total_samples)))\n",
    "print(\"所有正样本个数%d，占比%.2f\" % (sum_positive_samples, (sum_positive_samples / total_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples中所有文件长度： 128185\n",
      "filtered_negative_samples长度： 47386\n",
      "负样本剔除空内容之后文件长度： 47382\n",
      "\n",
      "\n",
      "positive_samples中所有文件长度： 56253\n",
      "filtered_positive_samples长度： 12738\n",
      "正样本剔除空内容之后文件长度： 12738\n",
      "\n",
      "\n",
      "包含空样本的负样本占比0.79：\n",
      "包含空样本的正样本占比0.21：\n",
      "\n",
      "\n",
      "实际负样本占比0.79\n",
      "实际正样本占比0.21\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    剔除结果中重复的文章\n",
    "'''\n",
    "with open(\"./filtered_wechat_negative_samples.txt\", \"w\", encoding=\"utf-8\") as fnf, open(\"./filtered_wechat_positive_samples.txt\", \"w\", encoding=\"utf-8\") as fpf:\n",
    "    with open(\"./wechat_negative_samples.txt\", \"r\", encoding=\"utf-8\") as nf, open(\"./wechat_positive_samples.txt\", \"r\", encoding=\"utf-8\") as pf:\n",
    "\n",
    "        nf_data = nf.readlines()\n",
    "        print(\"negative_samples中所有文件长度：\", len(nf_data))\n",
    "        print(\"filtered_negative_samples长度：\", len(set(nf_data)))\n",
    "        count1 = 0\n",
    "        for i in set(nf_data):\n",
    "            if len(i.strip().split(\"\\t\")) == 3:\n",
    "                fnf.write(i)\n",
    "                count1 += 1\n",
    "        print(\"负样本剔除空内容之后文件长度：\", count1)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        pf_data = pf.readlines()\n",
    "        print(\"positive_samples中所有文件长度：\", len(pf_data))\n",
    "        print(\"filtered_positive_samples长度：\", len(set(pf_data)))\n",
    "        count2 = 0\n",
    "        for j in set(pf_data):\n",
    "            if len(j.strip().split(\"\\t\")) == 3:\n",
    "                fpf.write(j)\n",
    "                count2 += 1\n",
    "        print(\"正样本剔除空内容之后文件长度：\", count2)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"包含空样本的负样本占比：%.2f\" %  (len(set(nf_data))/ (len(set(pf_data)) + len(set(nf_data)))))\n",
    "        print(\"包含空样本的正样本占比：%.2f\" %  (len(set(pf_data))/ (len(set(pf_data)) + len(set(nf_data)))))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"实际负样本占比：%.2f\" % (count1/(count1+count2)))\n",
    "        print(\"实际正样本占比：%.2f\" % (count2/(count1+count2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从wechat获取的正负样本，正样本保留，负样筛选，保留四分之一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11846\n"
     ]
    }
   ],
   "source": [
    "select_number = []\n",
    "for i in range(0, 47382, 4):\n",
    "    select_number.append(i)\n",
    "print(len(select_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11846\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    选大概四分之一的负样本保留下来，并剔除content为空的数据\n",
    "'''\n",
    "with open(\"./filtered_wechat_negative_samples.txt\", \"r\", encoding=\"utf-8\") as outf:\n",
    "    with open(\"./selected_wechat_negative_samples.txt\", \"w\", encoding=\"utf-8\") as inf:\n",
    "        data = outf.readlines()\n",
    "        count = 0\n",
    "        for i in select_number:\n",
    "            if len(data[i].strip().split(\"\\t\")) == 3: # 判断content是否为空\n",
    "                inf.write(data[i])\n",
    "                count += 1\n",
    "        print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
