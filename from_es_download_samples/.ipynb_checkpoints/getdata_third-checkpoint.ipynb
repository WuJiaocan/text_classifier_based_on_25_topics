{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    从*.bl2中批量获取四个平台的数据，wechat/baidu/toutiao/36kr，以保证样本特征多样性。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按主题词从es里重新获取wechat数据，划分正负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = [\"大赛\", \"比赛\", \"博览会\", \"研讨会\", \"论坛\", \"高峰论坛\", \"年会\", \"峰会\", \"大会\", \"融资\", \"培训\",\"沙龙\", \"入选\",  \n",
    "              \"分享\", \"报告\", \"理事会\", \"会议\", \"对接会\", \"组委会\", \"展览会\", \"演讲\", \"发布会\", \"挂牌仪式\", \"互访\", \"圆桌\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    从es里读取的content是html格式，需要解析成文本\n",
    "'''\n",
    "def sentencesMaker(html):\n",
    "    sentences = []\n",
    "    if not html or not html.strip():\n",
    "        return sentences\n",
    "    try:\n",
    "        from html.parser import unescape\n",
    "        html = unescape(html)\n",
    "\n",
    "        import justext\n",
    "        paragraphs = justext.justext(html, [])\n",
    "\n",
    "        cache_sentences = ''\n",
    "\n",
    "        for p in paragraphs:\n",
    "            sent = p.text.strip().replace('\\xa0', '').replace('\\u3000', '')\n",
    "            sent = sent.encode('gb2312', 'ignore').decode('gb2312').encode('gbk', 'ignore').decode('gbk')\n",
    "            if not sent:\n",
    "                continue\n",
    "\n",
    "            # 可能是含有名字，需要进一步处理\n",
    "            if len(cache_sentences) < 5:\n",
    "                cache_sentences += ' ' + sent\n",
    "            else:\n",
    "                sentences.append(cache_sentences.strip())\n",
    "                cache_sentences = sent\n",
    "\n",
    "        if not not cache_sentences:\n",
    "            sentences.append(cache_sentences.strip())\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    连接es\n",
    "'''\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "es = Elasticsearch('http://ip:port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从bl2获取的id的长度： 93\n",
      "length of total samples from ner: 92\n",
      "length of negative samples: 62\n",
      "length of positive samples: 30\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    之前是只有title或者content里包括25个主题词就取出来，现在是必须题目里包括关键词\n",
    "    取正、负样本，划分依据从之前的rel至少又3个变成rel中不同的per至少有3个\n",
    "'''\n",
    "\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "def search_bl2(from_number, off_size):\n",
    "    es_search_options = set_search_optional_bl2(from_number, off_size)\n",
    "    es_result = get_search_result(query=es_search_options, index=\"*_bl2\")\n",
    "    return es_result\n",
    "\n",
    "def search_ner(final_results, from_number, off_size):\n",
    "    es_search_options = set_search_optional_ner(final_results, from_number, off_size)\n",
    "    es_result = get_search_result(query=es_search_options, index='*_ner')\n",
    "    return es_result\n",
    "\n",
    "def get_id_list(es_result):\n",
    "    final_result = []\n",
    "    for item in es_result[\"hits\"][\"hits\"]: # \n",
    "        final_result.append(item[\"_id\"])\n",
    "    return final_result\n",
    "\n",
    "def get_search_result(query, index):\n",
    "    es_result = es.search(\n",
    "        body=query,\n",
    "        index=index,\n",
    "        doc_type='news'\n",
    "    )\n",
    "    return es_result\n",
    "\n",
    "def set_search_optional_bl2(from_number, off_size): # 必须题目里包括关键词\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "  \"from\":from_number, \"size\":off_size,\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\n",
    "          \"terms\": {\n",
    "            \"title\": [\n",
    "                \"圆桌\"\n",
    "            ]\n",
    "          }\n",
    "        },\n",
    "#         {\n",
    "#           \"terms\": {\n",
    "#             \"title\": [\n",
    "#                \"互访\", \"沙龙\", \"博览会\", \"展览会\", \"圆桌\", \"挂牌仪式\", \"发布会\",\n",
    "#                \"演讲\", \"组委会\", \"对接会\", \"会议\", \"理事会\", \"报告\", \"分享\", \"入选\",\n",
    "#                \"培训\", \"融资\", \"大会\", \"峰会\", \"年会\", \"高峰论坛\", \"论坛\", \"研讨会\", \"比赛\", \"大赛\"\n",
    "#             ]\n",
    "#           }\n",
    "#         }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "    return es_search_options\n",
    "\n",
    "def set_search_optional_ner(final_results, from_number, off_size):\n",
    "    # 检索选项\n",
    "    es_search_options = {\n",
    "        \"from\":from_number, \"size\":off_size,\n",
    "          \"query\": {\n",
    "            \"ids\":{\n",
    "              \"values\": final_results\n",
    "            }\n",
    "          }, \n",
    "          \"_source\":  [\"rel\"]\n",
    "    }    \n",
    "    return es_search_options\n",
    "\n",
    "\n",
    "def split_samples(data):\n",
    "    negative_samples = []\n",
    "    positive_samples = []\n",
    "    for i in data[\"hits\"][\"hits\"]:\n",
    "        \n",
    "        pers = set([rels.get(\"per\", None) for rels in i['_source'].get('rel',[])])      # 抽出来的rel中不同的人名的个数per\n",
    "        titles = set([rels.get(\"title\", None) for rels in i['_source'].get('rel', [])]) # rel中解析出来的title是否为空\n",
    "        \n",
    "        if len(pers) < 3 or \"\" in titles: # per小于3或者title有为空的\n",
    "            negative_samples.append(i[\"_id\"])  \n",
    "        else:\n",
    "            positive_samples.append(i[\"_id\"])\n",
    "            \n",
    "    return negative_samples, positive_samples\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__': \n",
    "    all_results_bl2 = search_bl2(0,1000)\n",
    "    \n",
    "    bl2_ids_results = get_id_list(all_results_bl2)  # 接着返回的bl2结果中，取出来ids，传给ner返回rel\n",
    "    print(\"从bl2获取的id的长度：\", len(bl2_ids_results))\n",
    "    \n",
    "    all_results_ner = search_ner(bl2_ids_results, 0, 1000)   # ner利用bl2返回的ids的数据，取rel \n",
    "    negative_samples, positive_samples = split_samples(all_results_ner)  # 查看ner返回的rel信息，划分正负样本的id\n",
    "    \n",
    "    print(\"length of total samples from ner:\", len(set(negative_samples)) + len(set(positive_samples)))\n",
    "    print(\"length of negative samples:\", len(set(negative_samples)))\n",
    "    print(\"length of positive samples:\", len(set(positive_samples)))\n",
    "    \n",
    "#    根据正负样本的id，去bl2返回的结果中，取回相应的content\n",
    "    with open(\"./negative_samples.txt\", \"a\", encoding=\"utf-8\") as nf, open(\"./positive_samples.txt\", \"a\", encoding=\"utf-8\") as pf:  \n",
    "        for item in all_results_bl2[\"hits\"][\"hits\"]:\n",
    "            if item[\"_id\"] in negative_samples:\n",
    "                nf.write(\"1\" + \"\\t\" + item[\"_id\"] + \"\\t\" + item[\"_source\"][\"title\"]+\"//\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")\n",
    "            if item[\"_id\"] in positive_samples:\n",
    "                pf.write(\"0\" + \"\\t\" + item[\"_id\"] + \"\\t\" + item[\"_source\"][\"title\"]+\"//\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#     with open(\"./negative_dasai.txt\", \"w\", encoding=\"utf-8\") as nf, open(\"./positive_dasai.txt\", \"w\", encoding=\"utf-8\") as pf:  \n",
    "#         for item in all_results_bl2[\"hits\"][\"hits\"]:\n",
    "#             if item[\"_id\"] in negative_samples:\n",
    "#                 nf.write(\"1\" + \"\\t\" + item[\"_id\"] + \"\\t\" + item[\"_source\"][\"title\"]+\"//\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")\n",
    "#             if item[\"_id\"] in positive_samples:\n",
    "#                 pf.write(\"0\" + \"\\t\" + item[\"_id\"] + \"\\t\" + item[\"_source\"][\"title\"]+\"//\" + \"\".join(sentencesMaker(item[\"_source\"][\"content\"])).replace(\"\\n\", \"\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples中所有文件长度： 20459\n",
      "filtered_negative_samples长度： 19389\n",
      "负样本剔除空内容之后文件长度： 19389\n",
      "\n",
      "\n",
      "positive_samples中所有文件长度： 4044\n",
      "filtered_positive_samples长度： 3622\n",
      "正样本剔除空内容之后文件长度： 3622\n",
      "\n",
      "\n",
      "包含空样本的负样本占比：0.84\n",
      "包含空样本的正样本占比：0.16\n",
      "\n",
      "\n",
      "实际负样本占比：0.84\n",
      "实际正样本占比：0.16\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    剔除结果中重复的文章\n",
    "'''\n",
    "with open(\"./filtered_negative_samples.txt\", \"w\", encoding=\"utf-8\") as fnf, open(\"./filtered_positive_samples.txt\", \"w\", encoding=\"utf-8\") as fpf:\n",
    "    with open(\"./negative_samples.txt\", \"r\", encoding=\"utf-8\") as nf, open(\"./positive_samples.txt\", \"r\", encoding=\"utf-8\") as pf:\n",
    "\n",
    "            nf_data = nf.readlines()\n",
    "            print(\"negative_samples中所有文件长度：\", len(nf_data))\n",
    "            print(\"filtered_negative_samples长度：\", len(set(nf_data)))\n",
    "            count1 = 0\n",
    "            for i in set(nf_data):\n",
    "                if len(i.strip().split(\"\\t\")) == 3:\n",
    "                    fnf.write(i)\n",
    "                    count1 += 1\n",
    "            print(\"负样本剔除空内容之后文件长度：\", count1)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            pf_data = pf.readlines()\n",
    "            print(\"positive_samples中所有文件长度：\", len(pf_data))\n",
    "            print(\"filtered_positive_samples长度：\", len(set(pf_data)))\n",
    "            count2 = 0\n",
    "            for j in set(pf_data):\n",
    "                if len(j.strip().split(\"\\t\")) == 3:\n",
    "                    fpf.write(j)\n",
    "                    count2 += 1\n",
    "            print(\"正样本剔除空内容之后文件长度：\", count2)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            print(\"包含空样本的负样本占比：%.2f\" %  (len(set(nf_data))/ (len(set(pf_data)) + len(set(nf_data)))))\n",
    "            print(\"包含空样本的正样本占比：%.2f\" %  (len(set(pf_data))/ (len(set(pf_data)) + len(set(nf_data)))))\n",
    "            print(\"\\n\")\n",
    "\n",
    "            print(\"实际负样本占比：%.2f\" % (count1/(count1+count2)))\n",
    "            print(\"实际正样本占比：%.2f\" % (count2/(count1+count2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试获取rel中per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    连接es\n",
    "'''\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "es = Elasticsearch('http://123.206.13.101:59200')\n",
    "\n",
    "doc_index = '*_ner'\n",
    "doc_type = 'news'\n",
    "\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"ids\":{\n",
    "      \"values\": [\"MzA3MzI4MjgzMw==_2650742342_5\",\n",
    "      \"4643dd433d0c74494430b0147a92d50b\"]\n",
    "    }\n",
    "  }, \n",
    "  \"_source\":  [\"rel\"]\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "# es.search可以批量输出\n",
    "# helpers.scan尽管设置了size，仍是全部输出\n",
    "\n",
    "full_data2 = es.search(index=doc_index, doc_type=doc_type, body=query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************\n",
      "凌小宁\n",
      "凌小宁\n",
      "凌小宁\n",
      "张宏江\n",
      "凌小宁\n",
      "凌小宁\n",
      "凌小宁\n",
      "凌小宁\n",
      "2\n",
      "******************\n",
      "王坚\n",
      "唐文斌\n",
      "路人王\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in full_data2[\"hits\"][\"hits\"]:\n",
    "    name = []\n",
    "    print(\"******************\")\n",
    "    for rel in i['_source'].get('rel',[]):\n",
    "        print(rel.get('per'))\n",
    "        name.append(rel.get('per'))\n",
    "    print(len(set(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'凌小宁', '张宏江'}\n",
      "{'唐文斌', '路人王', '王坚'}\n"
     ]
    }
   ],
   "source": [
    "for i in full_data2[\"hits\"][\"hits\"]:\n",
    "#     print(i)\n",
    "    pers = set([rels.get(\"per\", None) for rels in i['_source'].get('rel',[])])\n",
    "    print(pers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', '联合创始人', '主席'}\n"
     ]
    }
   ],
   "source": [
    "for i in full_data2[\"hits\"][\"hits\"]:\n",
    "#     print(i)\n",
    "    titles = set([rels.get(\"title\", None) for rels in i['_source'].get('rel', [])])\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if \"\" in titles:\n",
    "    print(0)\n",
    "else:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
